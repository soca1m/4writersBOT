"""
Research Agent - –Ω–∞—Ö–æ–¥–∏—Ç –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —á–µ—Ä–µ–∑ Tavily API
"""
import logging
import os
from typing import List, Dict, Any
from datetime import datetime

from src.workflows.state import OrderWorkflowState
from src.utils.llm_service import get_fast_model
from src.utils.prompt_loader import get_researcher_prompt

logger = logging.getLogger(__name__)


async def research_sources_node(state: OrderWorkflowState) -> dict:
    """
    –ò—â–µ—Ç –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ —Ç–µ–º–µ –∑–∞–∫–∞–∑–∞

    Args:
        state: –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ workflow

    Returns:
        –û–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
    """
    logger.info(f"üîç Researching sources for order {state['order_id']}...")

    requirements: Dict[str, Any] = state.get('requirements', {})
    main_topic: str = requirements.get('main_topic', state.get('order_description', ''))
    required_sources: int = requirements.get('required_sources', 2)

    if not main_topic:
        logger.error("No topic found for research")
        return {
            **state,
            "status": "research_failed",
            "error": "No topic specified for research",
            "agent_logs": state.get('agent_logs', []) + ["[researcher] ERROR: No topic for research"]
        }

    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ Tavily API
        tavily_api_key: str = os.getenv("TAVILY_API_KEY", "")
        if not tavily_api_key:
            logger.error("TAVILY_API_KEY not found in environment")
            return {
                **state,
                "status": "research_failed",
                "error": "TAVILY_API_KEY not configured",
                "agent_logs": state.get('agent_logs', []) + ["[researcher] ERROR: TAVILY_API_KEY not found"]
            }

        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º Tavily
        try:
            from tavily import TavilyClient
        except ImportError:
            logger.error("tavily-python package not installed")
            return {
                **state,
                "status": "research_failed",
                "error": "tavily-python package not installed",
                "agent_logs": state.get('agent_logs', []) + ["[researcher] ERROR: tavily-python not installed"]
            }

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã —Å –ø–æ–º–æ—â—å—é LLM
        logger.info("Generating search queries...")
        llm = get_fast_model()

        if not llm:
            logger.warning("LLM not available, using topic as-is")
            search_queries: List[str] = [main_topic]
        else:
            current_year: int = datetime.now().year
            min_year: int = current_year - 5  # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ —Å—Ç–∞—Ä—à–µ 5 –ª–µ—Ç

            research_prompt: str = get_researcher_prompt(
                main_topic=main_topic,
                required_sources=required_sources,
                min_year=min_year
            )

            response = await llm.ainvoke(research_prompt)
            queries_text: str = response.content.strip()

            # –ü–∞—Ä—Å–∏–º –∑–∞–ø—Ä–æ—Å—ã –∏–∑ –æ—Ç–≤–µ—Ç–∞
            search_queries = []
            for line in queries_text.split('\n'):
                line = line.strip()
                if line and (line[0].isdigit() or line.startswith('-')):
                    # –£–±–∏—Ä–∞–µ–º –Ω–æ–º–µ—Ä –∏ –±–µ—Ä—ë–º —Ç–µ–∫—Å—Ç
                    query: str = line.split('.', 1)[-1].strip() if '.' in line else line.strip('- ')
                    if query:
                        search_queries.append(query)

            if not search_queries:
                search_queries = [main_topic]

            logger.info(f"Generated {len(search_queries)} search queries")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Tavily –∫–ª–∏–µ–Ω—Ç
        tavily_client = TavilyClient(api_key=tavily_api_key)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –ø–æ –∫–∞–∂–¥–æ–º—É –∑–∞–ø—Ä–æ—Å—É
        all_sources: List[Dict[str, Any]] = []
        seen_urls: set = set()

        # –ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –¥–æ–º–µ–Ω—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        academic_domains: List[str] = [
            "edu",
            "scholar.google.com",
            "researchgate.net",
            "jstor.org",
            "springer.com",
            "sciencedirect.com",
            "wiley.com",
            "tandfonline.com",
            "sagepub.com",
            "apa.org",
            "nih.gov",
            "pubmed"
        ]

        current_year: int = datetime.now().year
        min_year: int = current_year - 5

        for query in search_queries[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 3 –∑–∞–ø—Ä–æ—Å–∞–º–∏
            logger.info(f"Searching: {query}")

            try:
                # –ü–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
                response = tavily_client.search(
                    query=query,
                    search_depth="advanced",
                    max_results=5,
                    include_domains=academic_domains
                )

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                for result in response.get('results', []):
                    url: str = result.get('url', '')

                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                    if url in seen_urls:
                        continue

                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º PDF –Ω–∞–ø—Ä—è–º—É—é (–æ–Ω–∏ –æ–±—ã—á–Ω–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
                    if url.endswith('.pdf'):
                        continue

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π –¥–æ–º–µ–Ω
                    is_academic: bool = any(domain in url.lower() for domain in academic_domains)
                    if not is_academic:
                        continue

                    title: str = result.get('title', '')
                    content: str = result.get('content', '')

                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –≥–æ–¥ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                    year: int = extract_year_from_content(content, title)

                    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≥–æ–¥—É –µ—Å–ª–∏ –º–æ–∂–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å
                    if year and year < min_year:
                        logger.info(f"Skipping old source: {title} ({year})")
                        continue

                    source: Dict[str, Any] = {
                        'title': title,
                        'url': url,
                        'content': content[:500],  # –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
                        'year': year if year else None,
                        'query_used': query
                    }

                    all_sources.append(source)
                    seen_urls.add(url)

                    # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –µ—Å–ª–∏ –Ω–∞–±—Ä–∞–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
                    if len(all_sources) >= required_sources * 2:
                        break

            except Exception as e:
                logger.warning(f"Error searching query '{query}': {e}")
                continue

            # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –µ—Å–ª–∏ –Ω–∞–±—Ä–∞–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
            if len(all_sources) >= required_sources * 2:
                break

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if not all_sources:
            logger.warning(f"No academic sources found for topic: {main_topic}")
            return {
                **state,
                "status": "insufficient_sources",
                "sources_found": [],
                "agent_logs": state.get('agent_logs', []) + [
                    f"[researcher] WARNING: No academic sources found for: {main_topic}"
                ]
            }

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        sources_to_use: List[Dict[str, Any]] = all_sources[:required_sources * 2]

        logger.info(f"‚úÖ Found {len(sources_to_use)} academic sources")

        return {
            **state,
            "status": "sources_found",
            "sources_found": sources_to_use,
            "agent_logs": state.get('agent_logs', []) + [
                f"[researcher] Found {len(sources_to_use)} academic sources using {len(search_queries)} queries"
            ]
        }

    except Exception as e:
        logger.error(f"Error researching sources for order {state['order_id']}: {e}")
        return {
            **state,
            "status": "research_failed",
            "error": f"Research error: {str(e)}",
            "agent_logs": state.get('agent_logs', []) + [
                f"[researcher] ERROR: {str(e)}"
            ]
        }


def extract_year_from_content(content: str, title: str) -> int:
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –∏–∑–≤–ª–µ—á—å –≥–æ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞

    Args:
        content: –¢–µ–∫—Å—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        title: –ó–∞–≥–æ–ª–æ–≤–æ–∫

    Returns:
        –ì–æ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∏–ª–∏ 0 –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω
    """
    import re

    current_year: int = datetime.now().year

    # –ò—â–µ–º –≥–æ–¥ –≤ —Ñ–æ—Ä–º–∞—Ç–µ 4 —Ü–∏—Ñ—Ä
    text: str = f"{title} {content}"
    years: List[int] = []

    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –≥–æ–¥–∞
    patterns: List[str] = [
        r'\b(20[0-2][0-9])\b',  # 2000-2029
        r'\((\d{4})\)',  # –ì–æ–¥ –≤ —Å–∫–æ–±–∫–∞—Ö
        r'(?:published|pub\.|copyright|¬©)\s*(\d{4})',  # –ü–æ—Å–ª–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    ]

    for pattern in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        for match in matches:
            year: int = int(match)
            if 2000 <= year <= current_year:
                years.append(year)

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∞–º—ã–π –ø–æ–∑–¥–Ω–∏–π –≥–æ–¥ (—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –≥–æ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏)
    if years:
        return max(years)

    return 0
